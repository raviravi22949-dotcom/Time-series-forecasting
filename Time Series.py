# -*- coding: utf-8 -*-
"""Data Table Display

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/data_table.ipynb
"""

import os
import json
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, backend as K
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.preprocessing import StandardScaler
from datetime import datetime
import matplotlib.pyplot as plt
import kerastuner as kt

# ---------------------------
# Global config
# ---------------------------
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

DATA_DIR = "data"
OUT_DIR = "output"
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(OUT_DIR, exist_ok=True)

N_STEPS = 1200
N_FEATURES = 6
SEQ_LEN = 48
HORIZON = 1
EPOCHS = 80
BATCH = 32

# ----------------------------------------------------------------------------
# 1) Synthetic Data
# ----------------------------------------------------------------------------
def generate_synthetic(n=N_STEPS, f=N_FEATURES):
    t = np.arange(n)
    data = np.zeros((n, f))
    for i in range(f):
        seasonal = 0.6*np.sin(2*np.pi*t/(24*(i+1))) + 0.3*np.sin(2*np.pi*t/(7*(i+2)))
        trend = 0.0004*t*(i+1)
        noise = 0.15*np.random.randn(n)
        if i > 0:
            coupling = 0.25*np.roll(data[:, i-1], 1)
        else:
            coupling = 0
        data[:, i] = seasonal + trend + noise + coupling
    df = pd.DataFrame(data, columns=[f"f{i}" for i in range(f)])
    df["target"] = df["f0"]*0.5 + df["f1"].shift(1).fillna(0)*0.3 + 0.1*np.random.randn(n)
    return df

# ----------------------------------------------------------------------------
# 2) Windows
# ----------------------------------------------------------------------------
def create_windows(values, seq_len=SEQ_LEN, horizon=1):
    X, y = [], []
    for i in range(len(values) - seq_len - horizon + 1):
        X.append(values[i:i+seq_len, :-1])
        y.append(values[i+seq_len+horizon-1, -1])
    return np.array(X), np.array(y).reshape(-1, 1)

# ----------------------------------------------------------------------------
# 3) Custom Attention Layer (FIXED __init__)
# ----------------------------------------------------------------------------
class TimeAttention(layers.Layer):
    def __init__(self, attn_dim=32, return_weights=False, **kwargs):
        super(TimeAttention, self).__init__(**kwargs)
        self.attn_dim = attn_dim
        self.return_weights = return_weights

    def build(self, input_shape):
        d = input_shape[-1]
        self.W1 = self.add_weight(shape=(d, self.attn_dim),
                                  initializer="glorot_uniform")
        self.W2 = self.add_weight(shape=(self.attn_dim, 1),
                                  initializer="glorot_uniform")
        self.b = self.add_weight(shape=(self.attn_dim,),
                                 initializer="zeros")
        super().build(input_shape)

    def call(self, inputs):
        u = tf.nn.tanh(tf.tensordot(inputs, self.W1, axes=[[2], [0]]) + self.b)
        scores = tf.squeeze(tf.tensordot(u, self.W2, axes=[[2], [0]]), -1)
        alphas = tf.nn.softmax(scores)
        context = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), axis=1)
        if self.return_weights:
            return context, alphas
        return context

    def get_config(self):
        config = super().get_config()
        config.update({"attn_dim": self.attn_dim, "return_weights": self.return_weights})
        return config

# ----------------------------------------------------------------------------
# 4) Build model
# ----------------------------------------------------------------------------
def build_model(hp):
    lstm_units = hp.Int("lstm_units", 32, 128, step=32)
    att_dim = hp.Int("att_dim", 16, 64, step=16)
    lr = hp.Choice("lr", [1e-2, 1e-3, 5e-4])

    inputs = layers.Input(shape=(SEQ_LEN, N_FEATURES))
    x = layers.LSTM(lstm_units, return_sequences=True)(inputs)
    context = TimeAttention(att_dim)(x)
    out = layers.Dense(1)(context)

    model = models.Model(inputs, out)
    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss="mse")
    return model

# ----------------------------------------------------------------------------
# 5) Bayesian Optimization Search
# ----------------------------------------------------------------------------
def tune_model(X, y):
    tuner = kt.BayesianOptimization(
        build_model,
        objective="val_loss",
        max_trials=10,
        directory="tuner",
        project_name="ts_attention"
    )
    tuner.search(X, y, validation_split=0.2, epochs=30, verbose=1)
    return tuner.get_best_hyperparameters()[0]

# ----------------------------------------------------------------------------
# 6) Train final model with best hyperparameters
# ----------------------------------------------------------------------------
def train_final_model(X, y, best_hp):
    model = build_model(best_hp)
    cb = [
        EarlyStopping(patience=15, restore_best_weights=True, monitor="val_loss"),
        ModelCheckpoint("best_model.h5", save_best_only=True)
    ]
    model.fit(X, y, validation_split=0.2, epochs=EPOCHS, batch_size=BATCH, callbacks=cb)
    return model

# ----------------------------------------------------------------------------
# 7) Extract Attention Weights
# ----------------------------------------------------------------------------
def get_attention(model, X):
    lstm_output = models.Model(model.input,
                               model.get_layer("lstm").output).predict(X)

    att_layer = model.get_layer("time_attention")
    W1, W2, b = att_layer.get_weights()

    u = np.tanh(np.tensordot(lstm_output, W1, axes=[[2],[0]]) + b)
    scores = np.squeeze(np.tensordot(u, W2, axes=[[2],[0]]), -1)
    alphas = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    alphas = alphas / np.sum(alphas, axis=-1, keepdims=True)
    return alphas

# ----------------------------------------------------------------------------
# 8) Plotting
# ----------------------------------------------------------------------------
def plot_attention(avg, fname):
    plt.figure(figsize=(10, 3))
    plt.imshow(avg[np.newaxis, :], aspect="auto")
    plt.colorbar()
    plt.title("Attention Heatmap")
    plt.savefig(fname)
    plt.close()

def plot_pred(y_true, y_pred, fname):
    plt.figure(figsize=(10,3))
    plt.plot(y_true)
    plt.plot(y_pred)
    plt.title("Actual vs Predicted")
    plt.savefig(fname)
    plt.close()

# ----------------------------------------------------------------------------
# MAIN PIPELINE
# ----------------------------------------------------------------------------
def main():
    df = generate_synthetic()
    df.to_csv(DATA_DIR + "/synthetic.csv", index=False)
    values = df.values

    X, y = create_windows(values)
    scaler = StandardScaler()
    X2 = X.reshape(-1, N_FEATURES)
    scaler.fit(X2)
    Xs = scaler.transform(X2).reshape(X.shape)

    print("Running Bayesian Optimization…")
    best_hp = tune_model(Xs, y)

    print("Training final model…")
    model = train_final_model(Xs, y, best_hp)

    y_pred = model.predict(Xs)

    plot_pred(y, y_pred, OUT_DIR + "/actual_vs_predicted.png")

    alphas = get_attention(model, Xs[:64])
    avg = alphas.mean(axis=0)
    plot_attention(avg, OUT_DIR + "/attention_heatmap.png")

    report = {
        "best_hyperparameters": best_hp.values,
        "final_mse": float(np.mean((y - y_pred) ** 2)),
    }

    with open(OUT_DIR + "/report.json", "w") as f:
        json.dump(report, f, indent=4)

    print("\n✔ COMPLETED — FULL REPORT GENERATED IN /output")

if __name__ == "__main__":
    main()